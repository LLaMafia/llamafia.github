# 2023.12.20


## Paper & Discussion: DPO vs RLHF?

DPO：Direct Preference Optimization 

RMB-PO：Reward-Model-Based Policy Optimization

A1: DPO替代RLHF可造成多一倍的性能损失，用dpo泛化能力会较rlhf弱得明显,Li.etc 表示 因为DPO 偷懒了，用empirical data distribution 替代了真实的distribution，

A2: 可以看看statistical rejection sampling，有类似结论,gap没有在optimal policy采样，也没有reward model做泛化，不能explicit知道在optimal policy下generate samples谁好谁坏.

A3: 还是RL的经典的state distribution shift问题

A4: 搞个10次rej sampling + DPO估计也能打平

A5: PPO就是调参有挑战

A6: offline distribution跟optimal policy distribution的domain shift，Xiong, etc ”Gibbs Sampling from Human Feedback“ 论文的Sec 6有详细讨论DPO和RSO。

A7: DPO的训练方式还是会存在training-test gap，即训练时teacher-forcing的方式与实际测试有差异。这点就不如基于PPO的RLHF。

## Paper: FUDAN NLP loraMoE

- [loraMoE](https://arxiv.org/abs/2312.09979)

推荐理由：通过修改了原始的MoE策略来适配多任务lora训练，在参数量基本不变的情况下显著改善了知识遗忘和multi task能力，对资源需求也很友好

## Paper：Weight subcloning: direct initialization of transformers using larger pretrained ones

- [Weight subcloning link](https://arxiv.org/abs/2312.09299)

- 推荐理由：部分实现了权重迁移，想法和Sheared Llama 有相似之处

## Paper：codeforces eval llms

- [Competition-Level Problems are Effective LLM Evaluators](https://arxiv.org/pdf/2312.02143.pdf)

推荐理由： codeforces 比赛挺好的，出新题避免了数据泄露,论文report了pass@k 这种 sampling 结果，目前在 code 上没看到
self-debug 相比多次 sampling 有太明显优势，只能说可以提高执行通过率，前面的错误生成的甚至会误导后续修改。推荐者测过一些
math program synthesis 任务，HumanEval 之类的纯 code 任务有些论文比如 self-debug 跑过，code 上 self-debug
稳定提升得引入额外信息比如单元测试之类的。

tips:对于编程能力而言，leetcode每两周发起的周赛题（全是新题），是个比HumanEval更靠谱的test set。

## Discussion：语言模型输出端Tied Embeddings/Coupled Embeddings

Q: 为什么目前开源模型几乎都不 share input & output embedding？
目前 llama、mistral、qwen、deepseek 之类开源模型都设置 tie_word_embeddings=False，相反在 Transformer 提出时乃至 LSTM 时代
Weight Tying 都是主流模型的选择，当时认为共享 embedding 让训练更快更稳定，因此 bert / gpt / t5 系列模型基本都共享了。现在
model & data size 增大后，是否共享 embedding 有什么区别吗？

A1(某大佬):预训练刚兴起时，在语言模型的输出端重用Embedding权重是很常见的操作，比如BERT、第一版的T5、早期的GPT，都使用了这个操作，这是因为当模型主干部分不大且词表很大时，Embedding层的参数量很可观，如果输出端再新增一个独立的同样大小的权重矩阵的话，会导致显存消耗的激增。不过随着模型参数规模的增大，Embedding层的占比相对变小了，加之《Rethinking
embedding coupling in pre-trained language models》等研究表明共享Embedding可能会有些负面影响，所以现在共享Embedding的做法已经越来越少了。主要是现在share图啥呢？省那一点可有可无的参数量几乎不会带来速度的提升，也不省多少显存，但极有可能降低了模型的能力上限，这个风险值得冒吗？T5开始就基本不share了，也是T5首先表明不share好些.

A2: 那个时候data都比较少，质量参差不齐，task比较简单，share parameter的话也能当成一种regularization阻止过拟合，当时language
modeling就真的只是一个算ppl的task，也没多少人想到有这么大的潜力

A3: 这里说了模型会在mlp里绕过去,不利于解释

A4:  qwen tech report 上面说，Embedding and output projection. Based on preliminary experimental findings, we have
opted for the untied embedding approach instead of tying the weights of input embedding
and output projection. This decision was made in order to achieve better performance with
the price of memory costs.

A5: 之前embedding其实还是会占不少参数的 但现在大了以后backbone占主体吧 share不share对于最后性能也没啥影响

## Discussion: int4 VS float4

Q: 有个问题请教，在推理时，int4和float4, 本质上int4的范围更大，float4的粒度更细。在最新的一些推理改进中，比较推崇float4.
问题来了：float4比int4在推理时有啥优势？或者权重表达上粒度比范围更有价值？

A1: float4/float8这种格式，和int4/int8比，他的精度间隔密度不一样，int4/int8是等距的，
而float4/float8在0附近精度最高。反正神经网是希望数据都在0附近别离得太远，所以这里可能float4/float8这种可能应该有优势

A2：以前似乎也看过，说有效权重主要是在零附近。反正神经网是希望数据都在0附近别离得太远， 这个有没有啥论文详细讨论的？

A3:更正一下，没有说0附近更好吧，这和量化是两回事.量化之所以采用float有优势，是因为学得的权重分布有钟形分布的状态，如果采用int，会导致0周围的量化误差过大，而钟形分布进一步导致整体量化误差更大.
所以此时用float有优势，整体目标是为了rounding-error尽可能小.但这个都是有trade-off的，需要看你们训得模型权重的具体range和dist.这种主要还是为了量化。

## Discussion: 长xml 数据训练

Q: 请教一下，想做的领域序列长度比较长（5万~
10万token），类似于XML结构数据，信息冗余度比较高（含有大量标签符号）。所以考虑用一套算法（类似MEGABYTE）将序列压缩后训练，并通过解码器还原为原格式。请问大家有了解针对LLM的比较成熟的编解码算法吗？
也尝试过手工做tokenization，降低文本冗余度，但是工程量非常大，所以希望找一套自动化程度比较高的编解码策略。比如做化学符号（仅举例可能不准）相关文本的训练。每个化学符号在原始文本中可能使用多个描述性标签（例如<pos>、<num>
...），这些标签包括符号的位置、数量和与其他符号的连接等信息。为了减少token数量，手动进行tokenizer处理，将特定符号直接写成字符串“Item.pos.num”的形式（随后可做BPE）。尽管这种方法大大降低了token数量，但由于存在数百种甚至上千种表达规则，无法通过简化来覆盖所有情况。因此，手工完成转换工作量非常庞大。把原始XML手工解析后做BPE，合并到词表。但是这样丢掉了大量特殊符号的处理逻辑。所以希望可以直接对原始XML端到端做压缩，并用压缩后的token训练LLM。

A1: 大在哪？训练tokenizer不是现成的代码吗？你直接用sentencepiece或者bytepiece无监督训练一个tokenizer，vocab_size是可以自己控制的。直接处理原始数据，不用人工转换。

A2：那真不如在词表里面加上所有可能的 character based 的 token

## Discussion: bf16, bfloat16?

Q ：拿Llama-2/Mistral做inference的时候是用float16还是bfloat16好啊？我看HuggingFace都是用float16，但这些模型本身都是bfloat16训练的？

A1：bf16，把bfloat16 weight转成float16会有损失

A2:bf16训练，fp16推理，效果损失有测量过吗？

A3:其实影响应该都不大,ar模型都比较量化鲁棒,之前经常用4bit跑long context,然后4bit都只掉一两个点.当然我Measure的task是code
尤其code对量化比较indifferent aws有篇paper也论证了https://dl.acm.org/doi/10.1145/3611643.3616302

## Discussion:  Multiturn prompt

Q: 有没有模型多轮对话的 prompt 直接例子啊，就是能够一个 string 输入给模型，不用再过一个 warpper 的这种。我想用多轮对话来测试
few-shot，听说这么做效果很好。但是找不到一个完整的模板，只有一些源码里面讲的 special token.

A(某大佬):
可参考 [FranxYao](https://github.com/FranxYao/chain-of-thought-hub/blob/main/spl/gsm8k/chat/few_shot_cot.chatml)
这个仓库的代码。之前的结论是当你有 few shot 的时候把它以多轮对话的形式输入给模型，而不是全部塞在一个问题里面，可以
consistently 地增加 format 的 following，也能微弱但 consistent 地提高一两个点。

## Discussion: Multilingual embedding

Q: Multi-lingual Text Embedding Model现在有哪些比较好的开源模型？（比如性能可以接近Cohere Multi-lingual)

A:可以用 multilingual-e5做baseline,可以参考 https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail
multilingual的还是挺难做的，加新语言的时候经常会导致旧语言掉点，

## Discussion: In-context pretraining 提升能力 VS 提升学习速度

Q: in-context pretraining这样的技术（https://arxiv.org/pdf/2310.10638.pdf） 是提升模型的能力还是学习的速度呢?

A1: 感觉in context这种方式真是神奇啊，好像以前的learning theory没太见到过这种方式。应该算是auto regression语言模型独有的东西？

A2:促使模型可以收敛到一个更为平坦的loss平面上，提升泛化能力吧。

A3:我觉得 in-context pretraining 的核心是人工增加了一个序列内部的 dependency。如果不是 in-context pretraining，则一个 4K 的
chunk 可能由多个互不相关的文本组成，文本之间没有显著 correlation ；但如果是 in-context pretraining 则文本之间会存在
correlation，于是后一段文字更有理由去 attend 远处的文字

A4:这么说起来的话，那是不是in-context learning更像MCMC了：https://arxiv.org/abs/2111.02080

A5： 1.大家把多个short拼接起来，也是打满窗口，提升学习速度。但不同的文本混合，通过分割符来区分做分别学习，其实还是分开学习。。
2 这里还是同样的样本，但模型学习的时候获取到了更多的知识。
这算是一种数据增强的方式吧

## ref:

- [苏神embedding 推导](https://spaces.ac.cn/archives/9698#mjx-eqn-eq%3Aloss)
- [A Walkthrough of A Mathematical Framework for Transformer Circuits](https://www.youtube.com/watch?v=KV5gbOmHbjU&t=9307s )
- [Qwen tech report](https://arxiv.org/abs/2309.16609)
- [DPO替代RLHF 作者知乎讨论](https://zhuanlan.zhihu.com/p/673047773)
- [Policy Optimization in RLHF](https://arxiv.org/abs/2312.10584)
- [Gibbs Sampling from Human Feedback](https://arxiv.org/abs/2312.11456)
- [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)