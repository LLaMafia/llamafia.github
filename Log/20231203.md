## 数字切开验证&&数字计算
Q: 请问什么工作实验验证过，把数字按 digit 切开再做数学？

A: https://arxiv.org/pdf/2310.02989.pdf, 这个工作有做过讨论, single-digit tokenization 对算数应当是有帮助的

A1: 还有这个工作 https://arxiv.org/pdf/2305.14201.pdf 比对了不同的 tokenizer

研究员1：MathGLM 的计算做得很好，做数学就应该这么做，一切的恐惧来源于压缩得不够；

研究员2：gpt4v把一张图上的table parse成markdown然后bold每个column最大的数；但是它甚至连bold max这个事都做不好， 最后还是会让他把table parse进py然后用py去做这事（ 主要是取决于用户会不会信任model做加减法（im not） if not 感觉最后无论如何用户也会让model去调python， 对于llm里出来的任何number都不是非常正确（ 毕竟复杂问题 人类也都是要用计算器了）

研究员3：合理的，只要让LLM deeply integrate w/ PL其实就行 类似于那种<work> scratch pad，可以在生成的过程中生成py call on the fly进行计算啥；目前认为没有必要真的教会模型复杂算数，模型只要会写code做这事就行。就像人类想做多位数计算可能是可以算对，但计算器按两下就行了没必要自己算。目前认为code表述的计算过程会比直接给一个计算结果的number更verify一些：只要code看起来没问题 用户就能更trust它的结果

研究员4：Gpt4v 目前认为是混了ocr table的数据，最近的测试发现4v 细粒度的table ocr 能力还是独一档，开源模型基本做不了。

研究员5：解析几何是 2024 年数学 reasoning 需要重点攻破的对象

## Instruction following 能力
Q: instruction following 的能力，特别是 follow 比较复杂的 instruction，这种能力经验上更多是要求基础模型本身呢，还是 instruction 的数据更占大头?

研究员1：base 模型决定了能 follow 多么复杂的 instruction， 对于比较弱的 base 模型，反而需要简单指令

研究员2：base 模型决定了天花板，以及 instruction tuning 的难度。同样的数据拿很强的 base 模型随便调一下可能就比弱一些的模型花很多心思调要更强（模型尺寸相近的情况下比如 7B 13B） 

研究员3：对于复杂的指令，比较弱的base模型，可能花很多心思准备sft的数据，效果都不太好。但是换个更强的base模型，有可能struggle的问题不存在

研究员4： instruct following的能力，做好对齐和没做好对齐 差别很大，尤其是强化

研究员5：如果考虑对齐的话，有逻辑上的困难。对智能体A做对齐，需要更高的智能体B发指令做评判。如果A已经是最高的智能体了，B就应该被A替换了。就成了A自我对齐。但这种对齐操作完全可以内化为A的训练的一部分，实际上也就没有额外的对齐了。不过，A能不能不需要对齐而自我进化也是个问题，也许是更深刻的问题

研究员6： 教练不一定需要比运动员强，那些得了金牌的选手们，每个都比自己的教练强，对于一件任务，如果 verification 的难度 < action 的难度，那弱一点的 critic 应该就能帮助强一点的 actor。大部分任务都是 verification 难度小于或者等于 action，但是现在 verification 还是卡在 hallucination 上。如果先生成一个 cot 再生成一个 reward，这个中间的 cot 会 halluscination 会非常严重，即使给了模型很细致的评分标准，要求 ta 一定要按照这个评分标准判，它一来可能读不懂这个评分标准，二来也会各种判错。

研究员7：现有的work都说明了如果真的一点external feedback都不给，不太可能指望model能self-improve（尤其是对于reasoning的task），self-improve的case（GPT-3.5/4下）其实somewhat有点像有一个非常强的critic model，还跟generation model share非常接近的input context

## 论文分享

### 《我在Performer中发现了Transformer-VQ的踪影》
https://kexue.fm/archives/9862

从Transformer-VQ的exp(QC⊤)形式中，笔者联想到了Performer，继而“顺藤摸瓜”地发现原来Performer可以视为Soft版的Transformer-VQ。进一步地，笔者尝试类比Performer的推导方法来重新导出Transformer-VQ，为其后的优化提供一些参考结果。

https://mp.weixin.qq.com/s/vecFIilryt3HNms4Ak2NFw

### Multimodal-benchmark paper

### 1. A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
https://mmmu-benchmark.github.io/

作者评语：我们也是为社区做一点微小的工作，让社区在develop多模态模型的时候有个更好的benchmark反应模型的能力。现在的VQA类的数据都图片类型太过单一，细节不够充分，再有就是对模型的core ability 比如难一点的reasoning反应不出来。所以我们就尝试建立了这么一个benchmark，也希望能在2024年看到更多的优秀的多模态模型出现

2. UniIR: Training and Benchmarking Universal Multimodal Information Retrievers
https://arxiv.org/abs/2311.17136

## 讨论&杂谈

D: LLama2 预测用greedy依然有少量case，预测结果不一样的么? 关键是同一个batch是一摸一样的数据就没事，是不一摸一样数据，就会有一些少量预测不一致。

F: batching会做padding, 所以model input其实是不一样的.

F1: 之前有一种情况是某一层的权重没有load进来，load的时候没有严格对齐，层的大小不一致

F2: 如果是hf自带的flash attention + left padding，结果会不一致，flash attention里实际上没用mask那个参数 训练是right padding所以等价 

D: LLama 的 tokenizer 会和 titoken 有很本质区别么？ChatGPT play ground 提供的 tokenizer 限制长度为 3200，然而 LLama 的 tokenizer 出来的结果高达 4202。一般这个长度差距不会超过几十，但是这种直接差几百，令人感觉很惊讶。

F1: 目前这个问题比较大，注意到特别是在非英语文本和代码上可能会相差一倍。

F2: 就是差很多，这是 llama 的问题之一。别用 llama，baichuan mistral glm yi 都比 llama 好。

F3: 使用 tiktoken 来限制 prompt length。tiktoken 的速度比起 llama 的 tokenizer 更快.

F4: The LLaMA tokenizer is a BPE model based on sentencepiece.

F5: tiktoken 的速度不会和 llama2 tokenizer 有显著区别，确实很好更改.

F6: llama数字每个词都切开，会切成2 1 9 9 6，gpt/bert，词表中有数字串，可能会切成  2  1996.

D: 可以马上看到如何tokenizer，https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/

D: 某个特定的domain用self-instruct 的效果并不太好，在思考如何提高生成数据的质量，特别是如果只考虑 focus 在特定的任务上

F: WizardLM系列的工作从深度和广度上同时扩展prompt，生成结果的Diversity时好时坏，但是比self-instruct强一些。后来wizard还有系列针对特定领域的改进，说不定更符合需求。

F1: 一个做法：以某个高质量的benchmark中的prompt为seed，通过self-instruct的方式获得一堆高质量的prompt定向提升某个子领域的性能。Empirical的结论是会造成严重的污染，即只在这个榜单上表现的很好，随便换个本领域的问题就不太行了。这么干刷榜可以，提性能就是自己骗自己了 

F2: 之前nazneen讲过huggingface也试过类似self-instruct这样自动或者半自动生成数据的办法，结论也是不加仔细处理的话效果并不好，处理几轮之后最后从时间和成本花费来说感觉还不如找一帮训练有素专家和model深度协作搞data

D: https://zhuanlan.zhihu.com/p/669976120? 这是记录的大模型SFT阶段训练不稳定的探索（LLama2框架下的）

## 简短问答

Q: 只调学习率，有的时候会 OOM 有的时候不会 OOM，应该是内存本身的波动导致的？这时候开 offloading 是不是就能解决，也不怎么影响速度呢？

A1: 单卡只能减小 micro-batch size 或者 开启 checkpointing （gradient recompute） ， 后者有30% 的计算开销

Q1-continue:  双卡zero-3gradient-checkpoint已经开了, 可以理解为偶尔把OOM的一小部分offload到cpu上开销应该不是很大吗？

A2: 可以用offload和nvme，还有如果数据没有全部load进去了，也可以优化一下。

Q2: 数据这块咋优化呀？如果是说输入数据的话，输入数据本身占的显存应该很少呀；如果是说激活值的话，除了 activation recomputation，还有什么办法吗？

A3: https://github.com/datawhalechina/thorough-pytorch/discussions/15 试试这个. 同时可以 试试开lora，实在不行用这段代码评估一下你的模型需要多少内存和显存：
python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'

Q:请问dpo和直接mle或者contrastive learning有什么区别呢？从梯度上来看这三者都可以写成\nabla log p_w - \nabla p_lose. 只是mle在第二项前加了一个权重, dpo是对每一个pairwise的data用sigmoid(p_l - p_w)做了加权

A: 就是加强版contrastive loss, 本质上是offline RL，和online还是会有差距的。

Q: 请问大家LLM的多语言训练这个方向有哪些文章呢? 最近观察到LLM不同语言的能力差异还是挺大的，有些同样的reasoning英文做得又快又好，中文就很差。有没有讨论关于跨语言知识迁移这种内容的paper呢?

A: 这篇有点相关https://arxiv.org/pdf/2210.03057.pdf

Q：deepseek 在最后两百 B 降 lr 的这个操作，是因为开始的时候 lr 设得太大了后期发现降不下来了吗？

A: 初始最大的学习率是通过超参scaling law的方式得到的，两阶段decay有点启发式（基本上我们训练大小模型都是这个方式），不是根据67B的loss临时改的

F: 会不会是因为 learning rate decay 到 30% 导致学习率到后期偏高学不动了，其实看 loss 有点收敛了？试过全程 decay 到 10% 

F1: 拿这些中间的 checkpoints 来分析 learning dynamics / grokking / emergent ability 应该能出特别多的 insights，之前 danqi 组有一篇文章在 OPT 做的 https://arxiv.org/abs/2212.09803

Q: 请问下4xA100 80G能full parameter训得动34B的模型吗?

A: 4卡的话应该13b都train不动

A1: 30B至少得8卡开model parallel

A2: 34B 模型， 训练需要的参数占显存是 34G * 16 = 544G 显存，只放 weight 就需要 7张 A100 80G 了。  极限情况下（ZeRO3 / TP ， Gradient Recomputation）8 卡 可能能起起来。

F: *8可以理解，再次*2是为啥？weight *2， grad *2 ， Optim state （fp32 weight、m、v） * 12

Q: 目前有哪些满足如下标准的 evaluation 数据集：knowledge intensive + reasoning intensive, 但又不是 calculation intensive呢？一个例子是 MMLU high school physics/chemistry，它们都涉及大量的物理化学专门知识点，同时也需要灵活地运用这些知识点完成多步推理才能做对题，但是涉及到的计算量并不算大。而像 MATH 就是 knowledge + reasoning + calculation intensive 了，有时候哪怕模型明白一道题需要什么知识点，也知道该怎么运用这个知识点来完成推理，但中间算错一步的话结果也就错了。

A: 其实w/o KB也不算特别离谱的setting，model也越来越强， LMM已经有这样setting的benchmark，比如yang做的https://open-vision-language.github.io/infoseek/



