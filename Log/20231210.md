## MOE Evaluation时的capacity

Q: 是不是可以考虑moe eval的时候capacity设置得高一点？

研究员1: infer的时候capacity设置很高也会导致训练测试不一致，现在设置的是无限大，不丢token。

研究员2：同意1，但需要在更多的 benchmark 上一起看，因为需要判断token 都挤在一个 expert 是因为这个 expert 在数学相关的问题上 specialize 了，还是整个模型坍缩到这一个 expert 了。比如测十个不同的任务，每个任务都会挤在一个 expert， 但是每个任务的 expert都不一样，这种应该算 ok；但如果所有任务都挤在同一个 expert，这种应该算 collapse。

研究员1：目前观测到的是，部分（极少）任务会有非常拥挤。

研究员3：推荐领域是MMOE，或者不同的任务只在不同的头传递gradient，这样能避免拥挤，比较有名的是阿里提出的star模型。

研究员4：所以flan moe才work，MoE需要在alignment过程中调整的东西更多一些，比如需要flan类似的data去adapt load balancing shift。

## 模型外推

研究员1: 我们做了一个100k + 的长文本评测集 InfiniteBench（https://github.com/OpenBMB/InfiniteBench），评测下来KimiChat确实在长文本有不少性能甚至超GPT-4，不过在长代码等方面还是gpt-4更胜一筹。

研究员2：rwkv没训过100k+的，拿来比较不大公平。不能外推不单纯是位置编码的问题。任何非local方法（即理论总感受野大小超过训练长度）都不保证外推。

研究员3: 没见过的数据无限外推，这个的确是很难的。但ntk这种反而能生效，不是反而说明位置编码的确也是生效的一部分么？

研究员2: ntk就体现了外推是一个同分布问题，ntk修改了base，使得测试阶段的q、k夹角一定程度上与训练同分布。

研究员3：最近的一个结论，从语言模型的局域性来考察这些方法。局域性，是指语言模型在推断下一个token时，明显更依赖于邻近的token。直接外推保持了局域性（0附近位置编码不变），效果差是因为引入了超出训练长度的位置编码；位置内插虽然没有外推位置编码，但扰乱了局域性（0附近位置编码被压缩为1/�），所以不微调效果也不好；而NTK-aware Scaled RoPE通过“高频外推、低频内插”隐含了两者优点，保证了局域性，又没有明显外推位置编码，所以不微调也有不错的效果。跟YaRN的分析基本一致。

研究员4：是的，"高频外插，低频内插"是正确的解释，因为低频在extrapolation的时候最容易爆炸，我们positional interpolation里面简单的synthetic example做出来也是这样。https://twitter.com/tydsh/status/1674436093356421120。

## LLM self-generating training data讨论

https://arxiv.org/abs/2306.15895

想就这这篇讨论下左脚踩右脚这种 self-instruct 方法。我目前小范围验证了一下类似的方法。这种 self-instruct 在具体的 benchmark 上的确可以涨点，特别是针对 open-domain generation task。反过来，对于 close-domain generation/classification，随机生成 examples -> finetune model itself 这种方式会出现很强的系统性 bias，比如 generate 出来的 examples 全都是同一个 label，finetune 后的模型就只能生成一个 label。现在思索一些可行的 mitigate 方法，比如做 label conditional example generation -> re-annotate labels -> finetune model itself，想请教有没有什么看法能够 mitigate self-instruct 方法在 classification 上的能力。

研究员1：分类问题是不是也应该加cot，否则就很intractable。

研究员2：可以试试用比较拉的模型生成数据，然后G4打标。因为G4的alignment做的太好了，反而限制了一些数据的生产。

研究员3：这个像是distill，如果是distill的话 不用更强的model没法做。除非引入一些别的信息 比如加一些reward model/清洗一些不行的generate data。

F3(Following3)：由弱到强 distill 和由弱到强 supervise 是很有意思的想法了。

研究员4：最近有看到一篇文章说用synthetic的medical data可以提高模型性能 (https://arxiv.org/abs/2311.09402)。

研究员5：思路有点类似self alignment，之前看到过：
[Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf)。

## Loss形状由训练数据的顺序决定

研究员1：不管训练多大的模型，只要样本次序一样，loss上的zigzag看起来就很相似。

研究员2: 同意，data顺序一样 这个现象很明显。

研究员3：loss的形状本质是这个batch的数据是否符合pretrainmodel的原始分布，也就是pretrain是否见过这批数据。

研究员4：不同的样本本身loss就不同，除了和模型相关更和数据本身的内在性质相关。比如代码loss就低，文言文loss就高。

Q4：是因为和搜索空间的大小有关吗，长度为T的2进制串预测的搜索空间是2^T，这比|V|^T要小的多（V是词表）。

A4：搜索空间就是整个词表，是和conditional相关。在pretrain中，0后面固定是1或者0。所以其他字符的prob就塌缩了。

研究员5：一样的loss curve并不令人惊讶。数据顺序定了之后，梯度更新的顺序也是确定的，每个batch产生的“影响”也是确定的。随着模型size的变大，NTK的假设越成立，此时，参数的初始化的影响也越小。我们有个还在审的工作讨论过2个batch之间在learning视角下的关系。有兴趣的朋友可以看https://openreview.net/forum?id=8Ju0VmvMCW。

## Mixtral讨论

D：有人测试过 mixtral-8x7b 了吗？

研究员1：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen，https://twitter.com/Francis_YAO_/status/1733686003687112983。

研究员2：猛。解决了计算量问题？显存还是差不多？

研究员3：显存应该没解决 但是inference应该是远快于50B Dense模型。

研究员4：显存MoE比Dense还是会更多的，但是Mistral 如果train个50B的dense绝对比这个结果好很多很多，这个结果大致相当于Mistral 13B。

A4：显存就是 50B dense 的显存需求。推理速度应该是 14B 的激活。

研究员4：比这个少吧，他每层都是MoE layer吗？而且top2 只加ffn的flops 不加attention吧。

A4：夹心的moe，7*8 不会有 50b。 config里有参数，具体得算一下。

研究员4：确实，那这么算这MoE训得也没有那么惊艳了。大量的routing带来的overhead, top-2 every layer带来额外的计算量，再加上更多的参数，可能还不如直接上Dense划算。我的OpenMoE-7B/32E，每四层放一个，一共34B。



## 论文分析
https://arxiv.org/pdf/2311.16452.pdf

本文介绍了一种通过构建提示让通用模型战胜专用模型的方法，运用在医药领域。https://arxiv.org/pdf/2311.10367v1.pdf 这篇有分析icl和instruction tuning的差异，同样的观点。

https://arxiv.org/abs/2307.12950 

RLCD，用positive/negative prompts生成data，这样data自动有pos/neg labels，然后train reward models。

Q：很有意思，最近看Anthropic网站也更新了不少自动构建数据的方法。想问一下，RLCD是如何保证p+ p-的质量呢？原始RLAIF是通过cot的方式进行critic and revise迭代的策略，似乎质量更可信一些？

A：RLCD只是给你two initial generations，并且likely两个generation的质量不同，不像RLAIF有可能被迫在两个非常相似的答案中选一个更好的（有时候哪个更好完全随机）。也可以用cot的方式revise让他们变得更接近但不一样。

（预训练数据集）https://huggingface.co/datasets/Skywork/SkyPile-150B
天工开源150B中文预训练语料安全审核后已重新开放。

https://github.com/AI-in-Health/MedLLMsPracticalGuide

关于Medical LLM的survey report。

https://www.anthropic.com/index/claude-2-1-prompting

Anthropic发现，通过在提示中添加一句话，可以将大型语言模型的recall能力提高70%：“Here is the most relevant sentence in the context:”。这足以将Claude 2.1在20万字的上下文窗口中的得分从27%提高到98%。

## 讨论&杂谈

D(Discussion)：为什么clip global grad norm的默认取值通常是1？这隐含了一个结论，即所有模型的global grad norm都是O(1)级别的。为什么这么说呢？如果global grad norm一直明显大于1，那么就不是clip而是l2 normalize了；如果global grad norm一直明显小于1，那么clip就没有作用了。所以默认等于1，必然假设默认情况下global grad norm都是1附近（有大于1、有小于1），实验结果也看到基本上都成立。

在什么样的假设之下，如何证明相当大的一类模型的global grad norm都是O(1)，或者至少初始grad norm是O(1)？

F1(Follow1)：这个可能和模型初始化和层数宽度这些layout相关 global grad norm设置为1是为了让初始化阶段不同size的model update都尽量保持O(1)， 这样可能会带来optimal lr/bsz在不同size的模型下可transfer。

F2(Follow2): 之前lamada优化器要求的是 grad / W < 1在每层transformer，如果grad大于原始权重会使得模型优化很不稳定，这就是为啥前期W小的时候要warm up。这个对grad norm方向的解释是蛮make sense的。

D：[GPT-4的tokenizer都换新了吗?](https://mp.weixin.qq.com/s/TERJth0Qy401LfAXiFXpIQ)。

F1：https://arxiv.org/abs/2311.07547也发现了，gpt4对于misspell鲁棒性非常好，有时候谐音梗黑都能看出来。

F2: 也不一定是换了tokenizer，也可能是关注到了词语级别之外的任务信息，搜集/构造/合成了相关数据。

F3：3.5也可以关注到这个，我觉得不一定是换了Tokenizer。我觉得应该是加了一些工程trick。

F4：可能这些都是data augmentation的一种，预训练的时候加进去训练过了。

D：有没有试过用 4090 做多机多卡 pretrain，这个的 relative performance 会比 A100 差多少？

F1：单卡比a100快，双4090比单卡慢，通信太慢，数量级差异。有人测试过，结论是推理还行，训练就算了（https://zhuanlan.zhihu.com/p/655402388)。

F2：4090不支持nvlink，卡间通讯上不去。

F3：deepspeed之前有评估过不同硬件对训练的影响。用其中F+R+Z3方案的话，7B 的模型单机八卡是 10 倍的差距。sft 是完全能接受的. https://arxiv.org/abs/2311.03687。结论就是 sft 的话 13b 模型应该完全能跑。

D： [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)，线性复杂度，scaling behavior 也跟 transformer match 上了。

F1：S6神奇的地方在于真的回到RNN了，Attention module不能Parallel Train，不过考虑到现在flashattention也实际是sequential的，所以也还合理。

F2：问题在于终归还只是一个线性rnn，记忆容量只有hidden size那么大。

F3：但看 induction head 表现比 transformer 好。

F4：连rwkv都开始改multi head（head_dim> 1，线性rnn约等于head_dim=1）来增大记忆容量了，显式decay一律不看好，另外它那个forget gate只依赖于输入，不依赖于h_(t-1)，虽然指标好了，但没改变实质。

D：apple 发的新机器学习框架，支持统一内存 （https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm）。

F1：已经测试了，Lora 微调个 Llama-7B，大概每秒看 250-300 个 Tokens。设备 M2 Max https://x.com/ipruning/status/1732353942288585143?s=20。具体文档与 Loss 记录 https://xnboqt31tz.feishu.cn/wiki/D5RMwkGZpiQLjrkSC1icF47Enfb?from=from_copylink。

推理速度和 llama.cpp 差不多。在M2 Max上推理Mistral-7B，llama.cpp 一秒钟大概 24 个 Tokens，
MLX 一秒钟大概 30 个 Tokens。大概快 15-30%。

F2：在mac studio m2 ultra上推理yi，速度是 34b 0.5t/s，gpu 占用率40%。结论是性能太低，无法替代gpu推理。

D：举个有意思的结论，window attention其实类似rnn，我发现如果模型全用window attention，那么window size要开到training length量级才追平full attention效果（比如mistral是training length的一半），这个量级之下，window attention带来的提速其实并不明显。但是，如果插入一两层full attention，也就是hwfa的用法，那么window size就可以开到非常小（比如不到128）而保持效果不变，这时候整个模型的复杂度就是近乎线性（kv cache也接近固定大小，因为大部分是window attention），而且保持了任意位置的retrieve能力。所以full attention很重要，哪怕只有一两层，纯rnn模型是有缺陷的。

F1：但是data dependency加上large hidden state改上去后表现越来越好了，可能还有空间。

A1：改变不了有损压缩的事实，以及mamba也有人测没啥特别提升。

F2：背书和翻书结合是最佳的，rnn只会背书，full attention只会翻书。只会背书记不全，只会翻书效率低。（进一步解释）人脑记忆容量是有限的，rnn的state是固定大小的，这两点很相似，看上去rnn就模仿了人的背书能力。attention类似于检索，kv cache是变长的，相当于看完就将它放在那里，有需要再去翻。

F3：https://arxiv.org/pdf/2212.14052.pdf 给个有意思的insight H3是Heyna的前身 它论文里的表1，2的synthetic dataset claim能够达到和attention一样的效果，但是复现代码你会发现H3的learning curve和Attention大不相同 Attention有一个grokking的loss骤降瞬间 但是H3是逐渐decay的。

D：想探讨下alignment阶段模型学习了什么。现在看下来认同度比较高的说法是base model决定了alignment的上限。我目前实验下来instruction data的内容而非形式，对alignment后的模型性能有很大的影响，但是给base model一些好的prompt似乎也能够解决（没有严格论证），所以我猜测模型没有真正的学习新知识（generation的能力），而是更好的把instruction和response关联（align）了起来，好的数据能更快促进这种alignment不知道理解的是否准确。

F1：我同意，模型更擅长学习的是上文到下文的映射关系，这和条件概率预测下一个词的训练也有关系。我有实验发现，哪怕不要instruction类型的数据，只要post-pretrain的数据里多一些能引导模型捕捉概率转移，隐含的语义关系的数据，也可以提升base model 的生成推理质量。

F2：icl和tuning 在某种意义是等效的，前几天讲论过。但是各有好坏。

F3：Base决定alignment上限该怎么理解？是指alignment tax嘛？我更多想的是Anthropic HH那篇提过，alignment tax只针对小模型。足够大的模型，对齐之后的performance反而更好。个人猜测理由是，更好的y|x可能模型原本generate不出来，但对齐之后就可以了，毕竟对齐的数据是通过对比y来修正y|x。

A3：是根据之前讨论的一些猜想。目前对齐阶段的数据量是远不如pretrain的，我想这个可能部分支持对齐有上限的结论，如果把instruction data scale up，不知道是否就能突破这个限制，不过这样可能就是continued pretrain了。

F4：我一直担忧的一个问题是，把instruction data scale up放入post train就有个问题，就是遗忘。

F5：最近也看到一篇说alignment其实可以不fine tuning，对比了base 和aligned model的token shift，发现align后只改变一些不与问题相关的引导词，然后用ICL 超出rlhf和sft不少(https://arxiv.org/pdf/2312.01552.pdf )。



## 简短问答

Q：请问为什么说FlashAttention是依赖量化的呀？

A：因为你把量化下掉以后，他就报错，说flash att要求fp16 or bf16。

Q：是不是可以考虑moe eval的时候capacity设置得高一点？

A：infer的时候capacity设置很高其实也会导致训练测试不一致，现在的设置是无限大，不丢token。

Q：大家对于怎么造GPT-4预训练级别的合成数据有参考文献吗？

A：字符乱序增强，gpt3论文里有讲。

Q：mac studio推理llama，怎么用得上加速功能？不是llama.cpp这种ggml框架。想用pytorch生态，想全精度 推理yi ，qwen等。

A：玩过 M2 Ultra，llama cpp 全精度 bf16 能搞。如果要在 torch 生态上要 Hack 一下，跑 llama-1要改 apply_rotary_emb 函数，llama-2 以及别的不太清楚。

Q：rlhf时多少reward等同于一次sft，这个reward的区间应该是怎么样的？

A：如果没有value model的情况下，baseline设置成0.0，reward = 1.0是一次sft。或者说不用PPO，设置成Reinforce的算法，reward = 1.0是一次sft。如果设置成有value model的，几乎对不上。

Q：说起来有人知道为什么视觉相关模型没有scale到 10B+ 的吗？

A：ViT-22B，vision palm 570b 。

Q：最近看到非常多关于benchmarking的文章，有没有工作做关于benchmark的benchmark，使得大家更够更好的评估每个benchmark侧重的能力？

A：Danqi Chen有一个评估LLM evalutor的工作：https://arxiv.org/abs/2310.07641。

Q：比较好奇Gemini-nano在手机上运行体感？

A：不用nano，Gemini pro体感已经很差了。随便测测各种hallucination / over rejection / verbosity很明显，体感不如之前的Bard，估计SFT RLHF没做扎实就匆忙release了。

Q：目前 有什么 好用的  moe 框架 推荐吗？

A：https://www.deepspeed.ai/tutorials/mixture-of-experts/，https://github.com/microsoft/tutel

Q：vicuna-7b-v1.5-16k这个模型是不是没有被做过RLHF？他们没有提到rlhf。

A：是的。

Q：搜了用stable diffusion或者controlnet来做类似风格迁移的task，但没发现比较符合我们要求的paper，我们这边数据比较少也比较特殊，应该是sd训练集里不包含的类型，试了ipadapter等效果不好，如果想control prompt image提取出的特殊特征做one/few shot style transfer，有什么比较好的想法吗？

A：https://arxiv.org/pdf/2211.09800.pdf，https://arxiv.org/abs/2308.07863，https://specialist-diffusion.github.io/，https://styledrop.github.io/。

Q：请教下怎么写综述。除了慢慢收集积累相关论文外，有啥方法把一个领域相关的文章较快找出来？

A：https://www.perplexity.ai/，perplexity为了 scientific literature 优化过。

A2：我也喜欢这个，AI+search是个好东西，有时候问GPT4的时候他长篇大论说半天你不知道他是否瞎说， perlexity给源头，源头往往说论文和网页，然后顺藤摸瓜容易找到需要的东西。

A3：再补充一个 https://elicit.com/。

Q：文章（https://zhuanlan.zhihu.com/p/664985891）说：第一阶段训练完成后模型训练了3.1T，中文占比39.6%。也就是说中文差不多训练了1.2T左右。另外，开源中文数据集有130B，请问这个数据集是规则过滤数据集，还是1:10下采样数据集？

A：直接采样的。



